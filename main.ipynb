{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e60a0fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c49adb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: indeks-standar-pencemaran-udara-(ispu)-tahun-2015-komponen-data.csv\n",
      "  Original columns: ['periode_data', 'tanggal', 'pm10', 'so2', 'co', 'o3', 'no2', 'max', 'critical', 'categori', 'lokasi_spku']\n",
      "  Standardized to: ['periode_data', 'tanggal', 'stasiun', 'pm_sepuluh', 'sulfur_dioksida', 'karbon_monoksida', 'ozon', 'nitrogen_dioksida', 'max', 'parameter_pencemar_kritis', 'kategori']\n",
      "  Rows: 365\n",
      "\n",
      "Loaded: indeks-standar-pencemaran-udara-(ispu)-tahun-2016-komponen-data.csv\n",
      "  Original columns: ['periode_data', 'tanggal', 'stasiun', 'pm10', 'so2', 'co', 'o3', 'no2', 'max', 'critical', 'categori']\n",
      "  Standardized to: ['periode_data', 'tanggal', 'stasiun', 'pm_sepuluh', 'sulfur_dioksida', 'karbon_monoksida', 'ozon', 'nitrogen_dioksida', 'max', 'parameter_pencemar_kritis', 'kategori']\n",
      "  Rows: 1830\n",
      "\n",
      "Loaded: indeks-standar-pencemaran-udara-(ispu)-tahun-2017-komponen-data.csv\n",
      "  Original columns: ['periode_data', 'tanggal', 'stasiun', 'pm10', 'so2', 'co', 'o3', 'no2', 'max', 'critical', 'categori']\n",
      "  Standardized to: ['periode_data', 'tanggal', 'stasiun', 'pm_sepuluh', 'sulfur_dioksida', 'karbon_monoksida', 'ozon', 'nitrogen_dioksida', 'max', 'parameter_pencemar_kritis', 'kategori']\n",
      "  Rows: 1825\n",
      "\n",
      "Loaded: indeks-standar-pencemaran-udara-(ispu)-tahun-2018-komponen-data.csv\n",
      "  Original columns: ['periode_data', 'tanggal', 'pm10', 'so2', 'co', 'o3', 'no2', 'max', 'critical', 'categori', 'lokasi_spku']\n",
      "  Standardized to: ['periode_data', 'tanggal', 'stasiun', 'pm_sepuluh', 'sulfur_dioksida', 'karbon_monoksida', 'ozon', 'nitrogen_dioksida', 'max', 'parameter_pencemar_kritis', 'kategori']\n",
      "  Rows: 365\n",
      "\n",
      "Loaded: indeks-standar-pencemaran-udara-(ispu)-tahun-2019-komponen-data.csv\n",
      "  Original columns: ['periode_data', 'tanggal', 'pm10', 'so2', 'co', 'o3', 'no2', 'max', 'critical', 'categori', 'lokasi_spku']\n",
      "  Standardized to: ['periode_data', 'tanggal', 'stasiun', 'pm_sepuluh', 'sulfur_dioksida', 'karbon_monoksida', 'ozon', 'nitrogen_dioksida', 'max', 'parameter_pencemar_kritis', 'kategori']\n",
      "  Rows: 345\n",
      "\n",
      "Total files loaded: 5\n"
     ]
    }
   ],
   "source": [
    "# Define column mapping to match data.md convention\n",
    "column_mapping = {\n",
    "    'pm10': 'pm_sepuluh',\n",
    "    'so2': 'sulfur_dioksida',\n",
    "    'co': 'karbon_monoksida',\n",
    "    'o3': 'ozon',\n",
    "    'no2': 'nitrogen_dioksida',\n",
    "    'critical': 'parameter_pencemar_kritis',\n",
    "    'categori': 'kategori',\n",
    "    'lokasi_spku': 'stasiun'\n",
    "}\n",
    "\n",
    "# Define column order to match data.md\n",
    "column_order = [\n",
    "    'periode_data', 'tanggal', 'stasiun', 'pm_sepuluh',\n",
    "    'sulfur_dioksida', 'karbon_monoksida', 'ozon', 'nitrogen_dioksida',\n",
    "    'max', 'parameter_pencemar_kritis', 'kategori'\n",
    "]\n",
    "\n",
    "\n",
    "final_year = 2019\n",
    "\n",
    "# Define the folder path and years to combine\n",
    "ispu_folder = Path(\"data/ISPU\")\n",
    "years_to_combine = [i for i in range(2015, final_year + 1, 1)]\n",
    "\n",
    "# List to store dataframes\n",
    "dfs = []\n",
    "\n",
    "# Load and standardize files that contain the desired years\n",
    "for file_path in ispu_folder.glob(\"*.csv\"):\n",
    "    # Check if any of the desired years is in the filename\n",
    "    for year in years_to_combine:\n",
    "        if str(year) in file_path.name:\n",
    "            df_temp = pd.read_csv(file_path)\n",
    "            \n",
    "            print(f\"Loaded: {file_path.name}\")\n",
    "            print(f\"  Original columns: {list(df_temp.columns)}\")\n",
    "            \n",
    "            # Rename columns to match convention\n",
    "            df_temp = df_temp.rename(columns=column_mapping)\n",
    "            \n",
    "            # Add missing columns with None\n",
    "            for col in column_order:\n",
    "                if col not in df_temp.columns:\n",
    "                    df_temp[col] = None\n",
    "            \n",
    "            # Reorder columns to match data.md\n",
    "            df_temp = df_temp[column_order]\n",
    "            \n",
    "            print(f\"  Standardized to: {list(df_temp.columns)}\")\n",
    "            print(f\"  Rows: {len(df_temp)}\\n\")\n",
    "            \n",
    "            dfs.append(df_temp)\n",
    "            break\n",
    "\n",
    "print(f\"Total files loaded: {len(dfs)}\")\n",
    "\n",
    "# Combine all dataframes\n",
    "df_combined = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d8634959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace empty strings and \"---\" with NULL (NaN)\n",
    "df_combined = df_combined.replace(['', '---', ' ', '  '], pd.NA)\n",
    "df_combined = df_combined[df_combined['kategori'] != 'TIDAK ADA DATA']\n",
    "\n",
    "df_combined['tanggal'] = pd.to_datetime(\n",
    "    df_combined['tanggal'], \n",
    "    format='mixed',  # Allows multiple formats\n",
    "    dayfirst=False,  # Month comes first in ambiguous cases\n",
    "    errors='raise'\n",
    ")\n",
    "\n",
    "# Convert pollutant columns to integers (pd.NA will remain as NaN for nullable Int64)\n",
    "numeric_columns = ['pm_sepuluh', 'sulfur_dioksida', \n",
    "                   'karbon_monoksida', 'ozon', 'nitrogen_dioksida', 'max']\n",
    "\n",
    "for col in numeric_columns:\n",
    "    df_combined[col] = pd.to_numeric(df_combined[col], errors='coerce').astype('Int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a6598819",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Function to extract station ID (DKIx format)\n",
    "def extract_station_id(station_name):\n",
    "    if pd.isna(station_name):\n",
    "        return station_name\n",
    "    # Extract DKI followed by a number\n",
    "    match = re.search(r'DKI\\d+', str(station_name), re.IGNORECASE)\n",
    "    if match:\n",
    "        return match.group(0).upper()\n",
    "    return station_name\n",
    "\n",
    "# Apply normalization to stasiun column\n",
    "df_combined['stasiun'] = df_combined['stasiun'].apply(extract_station_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "09456223",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined['ID'] = df_combined['tanggal'].astype(str) + '_' + df_combined['stasiun'].astype(str)\n",
    "df_combined = df_combined[['ID'] + column_order]\n",
    "df_combined = df_combined.sort_values(by=['tanggal']).reset_index(drop=True)\n",
    "df_combined = df_combined.ffill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ebacbea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.DataFrame'>\n",
      "RangeIndex: 4663 entries, 0 to 4662\n",
      "Data columns (total 12 columns):\n",
      " #   Column                     Non-Null Count  Dtype         \n",
      "---  ------                     --------------  -----         \n",
      " 0   ID                         4663 non-null   str           \n",
      " 1   periode_data               4663 non-null   int64         \n",
      " 2   tanggal                    4663 non-null   datetime64[us]\n",
      " 3   stasiun                    4663 non-null   str           \n",
      " 4   pm_sepuluh                 4663 non-null   Int64         \n",
      " 5   sulfur_dioksida            4663 non-null   Int64         \n",
      " 6   karbon_monoksida           4663 non-null   Int64         \n",
      " 7   ozon                       4663 non-null   Int64         \n",
      " 8   nitrogen_dioksida          4663 non-null   Int64         \n",
      " 9   max                        4663 non-null   Int64         \n",
      " 10  parameter_pencemar_kritis  4663 non-null   str           \n",
      " 11  kategori                   4663 non-null   str           \n",
      "dtypes: Int64(6), datetime64[us](1), int64(1), str(4)\n",
      "memory usage: 464.6 KB\n"
     ]
    }
   ],
   "source": [
    "df_combined.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e18ca0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.to_csv('data/ispu_2015-2019.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d382a65a",
   "metadata": {},
   "source": [
    "# QC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "668beb5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Data Quality Check ===\n",
      "\n",
      "Duplicate IDs: 345\n",
      "                  ID    tanggal stasiun kategori\n",
      "420  2016-01-12_DKI4 2016-01-12    DKI4   SEDANG\n",
      "421  2016-01-12_DKI3 2016-01-12    DKI3     BAIK\n",
      "423  2016-01-12_DKI3 2016-01-12    DKI3   SEDANG\n",
      "424  2016-01-12_DKI2 2016-01-12    DKI2   SEDANG\n",
      "425  2016-01-12_DKI5 2016-01-12    DKI5   SEDANG\n",
      "\n",
      "Missing values in critical columns:\n",
      "ID          0\n",
      "tanggal     0\n",
      "stasiun     0\n",
      "kategori    0\n",
      "dtype: int64\n",
      "\n",
      "Unique station IDs:\n",
      "['DKI1', 'DKI2', 'DKI3', 'DKI4', 'DKI5']\n",
      "\n",
      "Kategori distribution:\n",
      "kategori\n",
      "SEDANG                2938\n",
      "BAIK                   911\n",
      "TIDAK SEHAT            778\n",
      "SANGAT TIDAK SEHAT      36\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Invalid dates (NaT): 0\n",
      "\n",
      "Negative pollutant values:\n"
     ]
    }
   ],
   "source": [
    "# Check for issues that might need cleanup\n",
    "print(\"=== Data Quality Check ===\\n\")\n",
    "\n",
    "# 1. Check for duplicate IDs\n",
    "duplicates = df_combined[df_combined['ID'].duplicated(keep=False)]\n",
    "print(f\"Duplicate IDs: {len(duplicates)}\")\n",
    "if len(duplicates) > 0:\n",
    "    print(duplicates[['ID', 'tanggal', 'stasiun', 'kategori']].head())\n",
    "\n",
    "# 2. Check for missing values in key columns\n",
    "print(\"\\nMissing values in critical columns:\")\n",
    "print(df_combined[['ID', 'tanggal', 'stasiun', 'kategori']].isnull().sum())\n",
    "\n",
    "# 3. Check for invalid station IDs\n",
    "print(\"\\nUnique station IDs:\")\n",
    "print(sorted(df_combined['stasiun'].unique()))\n",
    "\n",
    "# 4. Check kategori values\n",
    "print(\"\\nKategori distribution:\")\n",
    "print(df_combined['kategori'].value_counts())\n",
    "\n",
    "# 5. Check for invalid dates\n",
    "print(f\"\\nInvalid dates (NaT): {df_combined['tanggal'].isna().sum()}\")\n",
    "\n",
    "# 6. Check for negative or unrealistic pollutant values\n",
    "print(\"\\nNegative pollutant values:\")\n",
    "for col in numeric_columns:\n",
    "    neg_count = (df_combined[col] < 0).sum()\n",
    "    if neg_count > 0:\n",
    "        print(f\"  {col}: {neg_count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
