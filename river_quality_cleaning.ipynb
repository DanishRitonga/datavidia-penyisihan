{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4177cc7",
   "metadata": {},
   "source": [
    "# River Water Quality Data Cleaning\n",
    "## DKI Jakarta - Data Integration Pipeline (2015-2024)\n",
    "\n",
    "This notebook processes and integrates river water quality data from multiple sources:\n",
    "- **PDF Tables**: 2015-2019 quarterly measurements\n",
    "- **PDF Table**: 2020 single measurement (October)\n",
    "- **CSV Files**: 2022-2024 monthly measurements\n",
    "\n",
    "**Output**: Unified, standardized dataset with daily interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec8886b",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6d17479",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import camelot\n",
    "import re\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from functools import reduce\n",
    "import json\n",
    "from shapely.geometry import Polygon, MultiPolygon\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3babb703",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bf915a",
   "metadata": {},
   "source": [
    "### 2.1 Station Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93f14e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "stasiun_mapping = {\n",
    "    'DKI1': 'Jakarta Pusat',\n",
    "    'DKI2': 'Jakarta Utara',\n",
    "    'DKI3': 'Jakarta Selatan',\n",
    "    'DKI4': 'Jakarta Timur',\n",
    "    'DKI5': 'Jakarta Barat'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a283593",
   "metadata": {},
   "source": [
    "### 2.2 Data Source Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5764c061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF: 2015-2019 data separation (year: number of periods)\n",
    "data_separation = {\n",
    "    2015: 3,  # 3 periods (quarterly)\n",
    "    2016: 2,\n",
    "    2017: 3,\n",
    "    2018: 4,\n",
    "    2019: 4\n",
    "}\n",
    "\n",
    "# CSV files\n",
    "csv_files = [\n",
    "    ('data/kualitas-air-sungai/sungai2022.csv', ';'),\n",
    "    ('data/kualitas-air-sungai/sungai2023.csv', ';'),\n",
    "    ('data/kualitas-air-sungai/data-kualitas-air-sungai-komponen-data.csv', ',')\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fb59d4",
   "metadata": {},
   "source": [
    "### 2.3 Parameter Standardization Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eff630a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "river_params_dict = {\n",
    "    \"total_dissolved_solids\": {\"params1\": \"TDS\", \"params2\": \"TDS\", \"params3\": \"ZAT PADAT TERLARUT TDS\"},\n",
    "    \"total_suspended_solids\": {\"params1\": \"TSS\", \"params2\": \"TSS\", \"params3\": \"ZAT PADAT TERSUSPENSI TSS\"},\n",
    "    \"ph\": {\"params1\": \"pH\", \"params2\": \"pH\", \"params3\": \"PH\"},\n",
    "    \"biological_oxygen_demand\": {\"params1\": \"BOD\", \"params2\": \"BOD\", \"params3\": \"BOD\"},\n",
    "    \"chemical_oxygen_demand\": {\"params1\": \"COD\", \"params2\": \"COD\", \"params3\": \"COD DICHROMAT\"},\n",
    "    \"cadmium\": {\"params1\": \"Cd\", \"params2\": \"Cd\", \"params3\": \"KADMIUM CD\"},\n",
    "    \"chromium_vi\": {\"params1\": \"Cr6+\", \"params2\": \"Cr6\", \"params3\": \"CROM HEXAVALEN CR6\"},\n",
    "    \"copper\": {\"params1\": \"Cu\", \"params2\": \"Cu\", \"params3\": \"TEMBAGA CU\"},\n",
    "    \"lead\": {\"params1\": \"Pb\", \"params2\": \"Pb\", \"params3\": \"TIMAH HITAM PB\"},\n",
    "    \"mercury\": {\"params1\": \"Hg\", \"params2\": \"Hg\", \"params3\": \"HG\"},\n",
    "    \"zinc\": {\"params1\": \"Zn\", \"params2\": \"Zn\", \"params3\": \"SENG ZN\"},\n",
    "    \"oil_and_grease\": {\"params1\": \"Minyak dan Lemak\", \"params2\": \"Minyak dan Lemak\", \"params3\": \"MINYAK DAN LEMAK\"},\n",
    "    \"mbas_detergent\": {\"params1\": \"MBAS\", \"params2\": \"MBAS\", \"params3\": \"MBAS\"},\n",
    "    \"total_coliform\": {\"params1\": \"Bakteri Koli\", \"params2\": \"Total Coliform\", \"params3\": \"TOTAL COLIFORM\"},\n",
    "    \"fecal_coliform\": {\"params1\": \"Bakteri Koli Tinja\", \"params2\": \"Fecal Coliform\", \"params3\": \"FECAL COLIFORM\"}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f2fa46",
   "metadata": {},
   "source": [
    "## 3. Geographic Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38ae2b0",
   "metadata": {},
   "source": [
    "### 3.1 Helper Functions for Boundary Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65ed6eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_boundary_sql(sql_content):\n",
    "    \"\"\"Parse SQL file to extract boundary data.\"\"\"\n",
    "    lines = sql_content.split('\\n')\n",
    "    data = []\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line.startswith(\"('\"):\n",
    "            try:\n",
    "                first_quote = line.find(\"'\")\n",
    "                second_quote = line.find(\"'\", first_quote + 1)\n",
    "                kode = line[first_quote+1:second_quote]\n",
    "                \n",
    "                third_quote = line.find(\"'\", second_quote + 1)\n",
    "                fourth_quote = line.find(\"'\", third_quote + 1)\n",
    "                nama = line[third_quote+1:fourth_quote]\n",
    "                \n",
    "                after_nama = line[fourth_quote+2:]\n",
    "                parts = after_nama.split(',', 2)\n",
    "                \n",
    "                if len(parts) >= 3:\n",
    "                    lat = parts[0].strip()\n",
    "                    lng = parts[1].strip()\n",
    "                    rest = parts[2]\n",
    "                    path_start = rest.find(\"'\")\n",
    "                    path_end = rest.rfind(\"'\")\n",
    "                    if path_start != -1 and path_end != -1:\n",
    "                        geom = rest[path_start+1:path_end]\n",
    "                        data.append([kode, nama, lat, lng, geom])\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    return pd.DataFrame(data, columns=['kode', 'nama', 'lat', 'lng', 'geom'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b6cd9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap_coords(coord_list):\n",
    "    \"\"\"Recursively swap [lat,lng] to [lng,lat] for GeoJSON compatibility.\"\"\"\n",
    "    if isinstance(coord_list[0], (int, float)):\n",
    "        return [coord_list[1], coord_list[0]]\n",
    "    else:\n",
    "        return [swap_coords(c) for c in coord_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9d57394",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_geometry(geom_str):\n",
    "    \"\"\"Parse geometry string to Shapely Polygon/MultiPolygon.\"\"\"\n",
    "    try:\n",
    "        coords = json.loads(geom_str)\n",
    "        coords = swap_coords(coords)\n",
    "        \n",
    "        if isinstance(coords[0][0], (int, float)):\n",
    "            return Polygon(coords)\n",
    "        elif isinstance(coords[0][0][0], (int, float)):\n",
    "            exterior = coords[0]\n",
    "            holes = coords[1:] if len(coords) > 1 else []\n",
    "            return Polygon(exterior, holes)\n",
    "        else:\n",
    "            polygons = []\n",
    "            for poly_coords in coords:\n",
    "                exterior = poly_coords[0]\n",
    "                holes = poly_coords[1:] if len(poly_coords) > 1 else []\n",
    "                polygons.append(Polygon(exterior, holes))\n",
    "            return MultiPolygon(polygons)\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e446be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_to_nearest_region(gdf_points, gdf_boundaries, nama_col='kota_kabupaten'):\n",
    "    \"\"\"Assign unassigned points to nearest boundary.\"\"\"\n",
    "    unassigned_mask = gdf_points[nama_col].isna()\n",
    "    unassigned_indices = gdf_points[unassigned_mask].index\n",
    "    \n",
    "    if len(unassigned_indices) == 0:\n",
    "        return gdf_points\n",
    "    \n",
    "    current_counts = gdf_points[nama_col].value_counts().to_dict()\n",
    "    \n",
    "    for idx in unassigned_indices:\n",
    "        point = gdf_points.loc[idx, 'geometry']\n",
    "        distances = []\n",
    "        \n",
    "        for _, boundary_row in gdf_boundaries.iterrows():\n",
    "            dist = point.distance(boundary_row['geometry'])\n",
    "            nama = boundary_row['nama']\n",
    "            count = current_counts.get(nama, 0)\n",
    "            distances.append((dist, count, nama))\n",
    "        \n",
    "        distances.sort(key=lambda x: (x[0], x[1]))\n",
    "        _, _, nearest_nama = distances[0]\n",
    "        \n",
    "        gdf_points.loc[idx, nama_col] = nearest_nama\n",
    "        current_counts[nearest_nama] = current_counts.get(nearest_nama, 0) + 1\n",
    "    \n",
    "    return gdf_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc24631c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_stasiun_column(df, stasiun_mapping):\n",
    "    \"\"\"Add 'stasiun' column based on kota_kabupaten mapping.\"\"\"\n",
    "    reverse_mapping = {v: k for k, v in stasiun_mapping.items()}\n",
    "    df['stasiun'] = df['kota_kabupaten'].map(reverse_mapping)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660bfdd5",
   "metadata": {},
   "source": [
    "### 3.2 Load DKI Jakarta Administrative Boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22d82994",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/cahyadsn/wilayah_boundaries/refs/heads/main/db/kab/wilayah_boundaries_kab_31.sql\"\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()\n",
    "sql_content = response.text\n",
    "\n",
    "# Clean SQL\n",
    "clean_sql = re.sub(r'^/\\*.*?\\*/\\s*', '', sql_content, flags=re.DOTALL | re.MULTILINE).strip()\n",
    "clean_sql = re.sub(r'--.*?$', '', clean_sql, flags=re.MULTILINE)\n",
    "clean_sql = re.sub(r'ENGINE=\\w+\\s*|DEFAULT CHARSET=\\w+\\s*', '', clean_sql, flags=re.IGNORECASE)\n",
    "clean_sql = clean_sql.replace('`', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9efba2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded 5 mainland DKI Jakarta boundaries\n"
     ]
    }
   ],
   "source": [
    "# Parse and filter boundaries (exclude Kepulauan Seribu)\n",
    "df_batas_wilayah = parse_boundary_sql(clean_sql)\n",
    "df_batas_wilayah = df_batas_wilayah[df_batas_wilayah['kode'] != '31.01'].reset_index(drop=True)\n",
    "\n",
    "print(f\"✓ Loaded {len(df_batas_wilayah)} mainland DKI Jakarta boundaries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15295135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created GeoDataFrame with 5 boundaries\n"
     ]
    }
   ],
   "source": [
    "# Convert to GeoDataFrame\n",
    "df_batas_wilayah['geometry'] = df_batas_wilayah['geom'].apply(parse_geometry)\n",
    "gdf_boundaries = gpd.GeoDataFrame(\n",
    "    df_batas_wilayah[df_batas_wilayah['geometry'].notna()], \n",
    "    geometry='geometry', \n",
    "    crs='EPSG:4326'\n",
    ")\n",
    "\n",
    "print(f\"✓ Created GeoDataFrame with {len(gdf_boundaries)} boundaries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f0e921",
   "metadata": {},
   "source": [
    "## 4. Sampling Locations Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b78a593",
   "metadata": {},
   "source": [
    "### 4.1 Extract Sampling Locations from PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c03d45c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Extracted 3 location tables from PDF\n"
     ]
    }
   ],
   "source": [
    "sampling_locs = camelot.read_pdf(\"airsungai_20250702121933.pdf\", pages=\"60-62\", flavor='lattice')\n",
    "print(f\"✓ Extracted {len(sampling_locs)} location tables from PDF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0cf6cec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_num_format(df, cols):\n",
    "    \"\"\"Convert European number format to standard format.\"\"\"\n",
    "    for col in cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = (\n",
    "                df[col].astype(str)\n",
    "                .str.replace('.', '', regex=False)\n",
    "                .str.replace(',', '.', regex=False)\n",
    "                .replace(['', 'nan', 'None'], pd.NA)\n",
    "            )\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9879224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Processed 111 sampling locations\n"
     ]
    }
   ],
   "source": [
    "sampling_locs_header = [\"No\",\"Kode\",\"Sungai\",\"Sub Jaringan\",\"Alamat\",\"DAS\",\"Lintang (DD)\",\"Bujur (DD)\",\"Lintang (DMS)\",\"Bujur (DMS)\"]\n",
    "\n",
    "slocs = []\n",
    "for sloc in sampling_locs:\n",
    "    df_sloc = sloc.df.iloc[2:]\n",
    "    df_sloc.columns = sampling_locs_header\n",
    "    df_sloc = df_sloc[['Kode', 'Lintang (DD)', 'Bujur (DD)']].rename(columns={'Lintang (DD)': 'lat', 'Bujur (DD)': 'lng'})\n",
    "    df_sloc = correct_num_format(df_sloc, ['lat', 'lng'])\n",
    "    slocs.append(df_sloc)\n",
    "\n",
    "slocs = pd.concat(slocs, ignore_index=True)\n",
    "print(f\"✓ Processed {len(slocs)} sampling locations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4b736a",
   "metadata": {},
   "source": [
    "### 4.2 Assign Sampling Locations to Stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a00c93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to GeoDataFrame and perform spatial join\n",
    "gdf_slocs = gpd.GeoDataFrame(slocs, geometry=gpd.points_from_xy(slocs.lng, slocs.lat), crs='EPSG:4326')\n",
    "\n",
    "gdf_slocs_with_region = gpd.sjoin(gdf_slocs, gdf_boundaries[['nama', 'geometry']], how='left', predicate='within')\n",
    "\n",
    "if 'index_right' in gdf_slocs_with_region.columns:\n",
    "    gdf_slocs_with_region = gdf_slocs_with_region.drop(columns=['index_right'])\n",
    "\n",
    "gdf_slocs_with_region = gdf_slocs_with_region.rename(columns={'nama': 'kota_kabupaten'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e501ed3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ All 112 sampling locations assigned to stations\n",
      "stasiun\n",
      "DKI1    12\n",
      "DKI2    14\n",
      "DKI3    27\n",
      "DKI4    36\n",
      "DKI5    23\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Assign to nearest boundary and add stasiun codes\n",
    "gdf_slocs_with_region = assign_to_nearest_region(gdf_slocs_with_region, gdf_boundaries, nama_col='kota_kabupaten')\n",
    "\n",
    "slocs_with_stasiun = pd.DataFrame(gdf_slocs_with_region.drop(columns=['lat', 'lng', 'geometry']))\n",
    "slocs_with_stasiun['kota_kabupaten'] = slocs_with_stasiun['kota_kabupaten'].str.replace('Kota Administrasi ', '')\n",
    "slocs_with_stasiun = add_stasiun_column(slocs_with_stasiun, stasiun_mapping).drop(columns=['kota_kabupaten'])\n",
    "\n",
    "print(f\"\\n✓ All {len(slocs_with_stasiun)} sampling locations assigned to stations\")\n",
    "print(slocs_with_stasiun['stasiun'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a8b96d",
   "metadata": {},
   "source": [
    "## 5. PDF Table Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5438c4b7",
   "metadata": {},
   "source": [
    "### 5.1 Load PDF Tables (2015-2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b355c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded 48 tables from PDF (2015-2019)\n"
     ]
    }
   ],
   "source": [
    "tables = camelot.read_pdf(\"airsungai_20250702121933.pdf\", pages=\"66-113\", flavor='lattice')\n",
    "print(f\"✓ Loaded {len(tables)} tables from PDF (2015-2019)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ba25ac",
   "metadata": {},
   "source": [
    "### 5.2 Load PDF Table (2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5fbdd4a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded 2020 table (split across 2 pages)\n"
     ]
    }
   ],
   "source": [
    "table2020 = camelot.read_pdf(\"airsungai_20250702121933.pdf\", pages=\"115\", flavor='lattice')\n",
    "print(f\"✓ Loaded 2020 table (split across {len(table2020)} pages)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a595e5",
   "metadata": {},
   "source": [
    "## 6. Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778c2458",
   "metadata": {},
   "source": [
    "### 6.1 Number Format Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8291efda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_range_values(value):\n",
    "    \"\"\"Handle range values like '<0.002', '>10', '≤5', etc.\"\"\"\n",
    "    if pd.isna(value) or value == '':\n",
    "        return value\n",
    "    value_str = str(value)\n",
    "    value_str = re.sub(r'^[<>≤≥\\s]+|[<>=≤≥\\s]+$', '', value_str)\n",
    "    return value_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5fdb66bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_european_number_format(df, cols):\n",
    "    \"\"\"Convert European number format and handle range values.\"\"\"\n",
    "    for col in cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].apply(handle_range_values)\n",
    "            df[col] = (\n",
    "                df[col].astype(str)\n",
    "                .str.replace('.', '', regex=False)\n",
    "                .str.replace(',', '.', regex=False)\n",
    "                .replace(['', 'nan', 'None'], pd.NA)\n",
    "            )\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607521af",
   "metadata": {},
   "source": [
    "### 6.2 Data Structure Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "99a1f79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatial_join_with_boundaries(df, lat_col, lng_col, gdf_boundaries, stasiun_mapping):\n",
    "    \"\"\"Perform spatial join to assign stations to coordinates.\"\"\"\n",
    "    gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df[lng_col], df[lat_col]), crs='EPSG:4326')\n",
    "    \n",
    "    gdf_with_region = gpd.sjoin(gdf, gdf_boundaries[['nama', 'geometry']], how='left', predicate='within')\n",
    "    \n",
    "    if 'index_right' in gdf_with_region.columns:\n",
    "        gdf_with_region = gdf_with_region.drop(columns=['index_right'])\n",
    "    \n",
    "    gdf_with_region = gdf_with_region.rename(columns={'nama': 'kota_kabupaten'})\n",
    "    gdf_with_region = assign_to_nearest_region(gdf_with_region, gdf_boundaries, nama_col='kota_kabupaten')\n",
    "    \n",
    "    df_with_region = pd.DataFrame(gdf_with_region.drop(columns=['geometry']))\n",
    "    df_with_region['kota_kabupaten'] = df_with_region['kota_kabupaten'].str.replace('Kota Administrasi ', '')\n",
    "    df_with_region = add_stasiun_column(df_with_region, stasiun_mapping)\n",
    "    \n",
    "    return df_with_region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "507de603",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pivot_and_merge_parameters(df, index_columns, parameter_column, value_column, param_types):\n",
    "    \"\"\"Pivot and merge different parameter types.\"\"\"\n",
    "    dfs = []\n",
    "    for param_type in param_types:\n",
    "        df_filtered = df[df['jenis_parameter'] == param_type]\n",
    "        df_pivoted = df_filtered.pivot_table(\n",
    "            index=index_columns,\n",
    "            columns=parameter_column,\n",
    "            values=value_column\n",
    "        )\n",
    "        df_pivoted = df_pivoted.reset_index()\n",
    "        dfs.append(df_pivoted)\n",
    "    \n",
    "    df_merged = reduce(lambda left, right: pd.merge(left, right, on=index_columns, how='inner'), dfs)\n",
    "    df_merged.columns.name = None\n",
    "    \n",
    "    return df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ed1c0a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_date_skeleton(years, stations):\n",
    "    \"\"\"Create skeleton dataframe with all date-station combinations.\"\"\"\n",
    "    skeleton_data = []\n",
    "    \n",
    "    for year in years:\n",
    "        date_range = pd.date_range(start=f'{year}-01-01', end=f'{year}-12-31', freq='D')\n",
    "        for date in date_range:\n",
    "            for stasiun_code in stations:\n",
    "                skeleton_data.append({'tanggal': date, 'stasiun': stasiun_code})\n",
    "    \n",
    "    skeleton_df = pd.DataFrame(skeleton_data)\n",
    "    return skeleton_df.sort_values(['tanggal', 'stasiun']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "421a746b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_temporal_data(df, skeleton, value_columns):\n",
    "    \"\"\"Expand sparse data to daily using forward/backward fill.\"\"\"\n",
    "    df_expanded = skeleton.merge(df[['tanggal', 'stasiun'] + value_columns], on=['tanggal', 'stasiun'], how='left')\n",
    "    df_expanded[value_columns] = df_expanded.groupby('stasiun')[value_columns].transform(lambda x: x.bfill().ffill())\n",
    "    return df_expanded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be98771",
   "metadata": {},
   "source": [
    "## 7. Data Processing Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bd902b",
   "metadata": {},
   "source": [
    "### 7.1 PDF Data Processing (2015-2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dedbec2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf_river_data(tables, data_separation, slocs_with_stasiun, river_params_dict):\n",
    "    \"\"\"Process river quality data from PDF tables (2015-2019).\"\"\"\n",
    "    numeric_cols = [\"TDS\", \"TSS\", \"pH\", \"BOD\", \"COD\", \"Cd\", \"Cr6+\", \"Cu\", \"Pb\", \"Hg\", \"Zn\", \n",
    "                    \"Minyak dan Lemak\", \"MBAS\", \"Bakteri Koli\", \"Bakteri Koli Tinja\"]\n",
    "    headers = [\"no\", \"Kode\", \"Sungai\", \"DAS\", *numeric_cols]\n",
    "    param_rename_map = {v['params1']: k for k, v in river_params_dict.items()}\n",
    "    \n",
    "    processed_tables = 0\n",
    "    tabs = []\n",
    "    \n",
    "    for year, num_periods in data_separation.items():\n",
    "        for period in range(1, num_periods + 1):\n",
    "            table_indices = [processed_tables, processed_tables + 1, processed_tables + 2]\n",
    "            period_dfs = []\n",
    "            \n",
    "            for idx in table_indices:\n",
    "                table = tables[idx].df.iloc[5:].copy()\n",
    "                table.columns = headers\n",
    "                table = correct_european_number_format(table.drop(columns=[\"no\"]), numeric_cols)\n",
    "                table = table.merge(slocs_with_stasiun[['Kode', 'stasiun']], on='Kode', how='left')\n",
    "                table = table.rename(columns=param_rename_map)\n",
    "                period_dfs.append(table)\n",
    "            \n",
    "            period_combined = pd.concat(period_dfs, ignore_index=True)\n",
    "            period_agg = period_combined.groupby('stasiun').agg({col: 'mean' for col in param_rename_map.values()}).reset_index()\n",
    "            period_agg['tahun'] = year\n",
    "            period_agg['periode'] = period\n",
    "            tabs.append(period_agg)\n",
    "            processed_tables += 3\n",
    "    \n",
    "    river_quality_all = pd.concat(tabs, ignore_index=True)\n",
    "    \n",
    "    # Add date column\n",
    "    month_map = {1: 3, 2: 6, 3: 9, 4: 12}\n",
    "    river_quality_all['bulan'] = river_quality_all['periode'].map(month_map)\n",
    "    river_quality_all['tanggal'] = pd.to_datetime(\n",
    "        river_quality_all['tahun'].astype(str) + '-' + river_quality_all['bulan'].astype(str) + '-15'\n",
    "    )\n",
    "    \n",
    "    param_cols = list(param_rename_map.values())\n",
    "    return river_quality_all[['tanggal', 'stasiun'] + param_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbb9004",
   "metadata": {},
   "source": [
    "### 7.2 2020 Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bcb68de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_2020_river_data(table2020, slocs_with_stasiun, river_params_dict):\n",
    "    \"\"\"Process river quality data from 2020 PDF table (single October measurement).\"\"\"\n",
    "    table2020_params = [\n",
    "        [\"Suhu\", \"TDS\", \"TSS\", \"pH\", \"BOD\", \"COD\", \"Total-P\", \"NO3\", \"Cd\", \"Cr6+\", \"Cu\", \"Pb\"], \n",
    "        [\"Hg\", \"Zn\", \"Flourida\", \"NO2\", \"Klorin Bebas\", \"H2S\", \"Minyak dan Lemak\", \"MBAS\", \"Fenol\", \"Bakteri Koli Tinja\", \"Bakteri Koli\"]\n",
    "    ]\n",
    "    table2020_index = [\"No\", \"Kode\", \"Sungai\", \"DAS\"]\n",
    "    \n",
    "    # Parse both tables\n",
    "    sungai2020 = []\n",
    "    for t in range(len(table2020)):\n",
    "        df = table2020[t].df.iloc[5:].copy()\n",
    "        df.columns = table2020_index + table2020_params[t]\n",
    "        sungai2020.append(df)\n",
    "    \n",
    "    # Merge and process\n",
    "    sungai2020_df = reduce(lambda left, right: pd.merge(left, right, on=table2020_index, how='inner'), sungai2020)\n",
    "    all_params = [col for sublist in table2020_params for col in sublist]\n",
    "    sungai2020_df = correct_european_number_format(sungai2020_df, all_params)\n",
    "    sungai2020_df = sungai2020_df.merge(slocs_with_stasiun[['Kode', 'stasiun']], on='Kode', how='left')\n",
    "    \n",
    "    # Standardize parameter names\n",
    "    param_rename_map = {v['params1']: k for k, v in river_params_dict.items()}\n",
    "    sungai2020_df = sungai2020_df.rename(columns=param_rename_map)\n",
    "    \n",
    "    # Aggregate by station\n",
    "    agg_cols = [col for col in sungai2020_df.columns if col in river_params_dict.keys()]\n",
    "    sungai2020_agg = sungai2020_df.groupby('stasiun').agg({col: 'mean' for col in agg_cols}).reset_index()\n",
    "    sungai2020_agg['tanggal'] = pd.to_datetime('2020-10-15')\n",
    "    \n",
    "    return sungai2020_agg[['tanggal', 'stasiun'] + agg_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6a3096",
   "metadata": {},
   "source": [
    "### 7.3 CSV Data Processing (2022-2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a10057d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_csv_river_data(csv_file, gdf_boundaries, stasiun_mapping, river_params_dict, delimiter=';'):\n",
    "    \"\"\"Process river quality data from CSV file.\"\"\"\n",
    "    df = pd.read_csv(csv_file, sep=delimiter)\n",
    "    \n",
    "    # Standardize column names\n",
    "    rename_map = {}\n",
    "    if 'latitude' in df.columns:\n",
    "        rename_map['latitude'] = 'lintang'\n",
    "    elif 'lintang_selatan' in df.columns:\n",
    "        rename_map['lintang_selatan'] = 'lintang'\n",
    "    \n",
    "    if 'longitude' in df.columns:\n",
    "        rename_map['longitude'] = 'bujur'\n",
    "    elif 'bujur_timur' in df.columns:\n",
    "        rename_map['bujur_timur'] = 'bujur'\n",
    "    \n",
    "    if 'periode_data' in df.columns and 'tahun' not in df.columns:\n",
    "        rename_map['periode_data'] = 'tahun'\n",
    "    \n",
    "    if rename_map:\n",
    "        df = df.rename(columns=rename_map)\n",
    "    \n",
    "    if 'periode_data' in df.columns and 'tahun' in df.columns:\n",
    "        df = df.drop(columns=['periode_data'])\n",
    "    \n",
    "    # Correct number formats\n",
    "    df = correct_european_number_format(df, ['lintang', 'bujur', 'hasil_pengukuran'])\n",
    "    \n",
    "    required_cols = ['tahun', 'bulan_sampling', 'lintang', 'bujur', 'jenis_parameter', 'parameter', 'hasil_pengukuran']\n",
    "    df = df[required_cols]\n",
    "    \n",
    "    # Aggregate duplicates\n",
    "    df = df.groupby(['tahun', 'bulan_sampling', 'lintang', 'bujur', 'jenis_parameter', 'parameter'], as_index=False)['hasil_pengukuran'].mean()\n",
    "    \n",
    "    # Pivot parameter types\n",
    "    param_types = ['KIMIA', 'FISIKA', 'BIOLOGI']\n",
    "    index_cols = ['tahun', 'bulan_sampling', 'lintang', 'bujur']\n",
    "    df_pivoted = pivot_and_merge_parameters(df, index_cols, 'parameter', 'hasil_pengukuran', param_types)\n",
    "    \n",
    "    # Standardize parameter names\n",
    "    param_rename_map = {v['params3']: k for k, v in river_params_dict.items()}\n",
    "    df_pivoted = df_pivoted.rename(columns=param_rename_map)\n",
    "    available_params = [col for col in df_pivoted.columns if col in river_params_dict.keys()]\n",
    "    df_pivoted = df_pivoted[index_cols + available_params]\n",
    "    \n",
    "    # Create date column\n",
    "    if df_pivoted['bulan_sampling'].astype(str).str.len().max() >= 6:\n",
    "        df_pivoted['tanggal'] = (\n",
    "            df_pivoted['bulan_sampling'].astype(str).str[:4] + '-' + \n",
    "            df_pivoted['bulan_sampling'].astype(str).str[4:] + '-15'\n",
    "        )\n",
    "    else:\n",
    "        df_pivoted['tanggal'] = (\n",
    "            df_pivoted['tahun'].astype(str) + '-' + \n",
    "            df_pivoted['bulan_sampling'].astype(str).str.zfill(2) + '-15'\n",
    "        )\n",
    "    \n",
    "    df_pivoted['tanggal'] = pd.to_datetime(df_pivoted['tanggal'], format='%Y-%m-%d')\n",
    "    \n",
    "    # Spatial join to assign stations\n",
    "    df_with_stasiun = spatial_join_with_boundaries(df_pivoted, 'lintang', 'bujur', gdf_boundaries, stasiun_mapping)\n",
    "    \n",
    "    # Drop unnecessary columns\n",
    "    cols_to_drop = ['tahun', 'bulan_sampling', 'lintang', 'bujur', 'kota_kabupaten']\n",
    "    df_with_stasiun = df_with_stasiun.drop(columns=[col for col in cols_to_drop if col in df_with_stasiun.columns])\n",
    "    \n",
    "    param_cols = [col for col in df_with_stasiun.columns if col not in ['tanggal', 'stasiun']]\n",
    "    return df_with_stasiun[['tanggal', 'stasiun'] + param_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70e37f1",
   "metadata": {},
   "source": [
    "## 8. Unified Data Integration Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3d725f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "RIVER WATER QUALITY DATA INTEGRATION PIPELINE\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"RIVER WATER QUALITY DATA INTEGRATION PIPELINE\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9b6755",
   "metadata": {},
   "source": [
    "### Step 1: Process PDF Data (2015-2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "edbe999b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1/6] Processing PDF data (2015-2019)...\n",
      "  → Shape: (80, 17)\n",
      "  → Date range: 2015-03-15 00:00:00 to 2019-12-15 00:00:00\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[1/6] Processing PDF data (2015-2019)...\")\n",
    "river_pdf = process_pdf_river_data(tables, data_separation, slocs_with_stasiun, river_params_dict)\n",
    "print(f\"  → Shape: {river_pdf.shape}\")\n",
    "print(f\"  → Date range: {river_pdf['tanggal'].min()} to {river_pdf['tanggal'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f672298",
   "metadata": {},
   "source": [
    "### Step 2: Process 2020 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c32a071c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2/6] Processing 2020 PDF data...\n",
      "  → Shape: (3, 17)\n",
      "  → Date: 2020-10-15 00:00:00\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[2/6] Processing 2020 PDF data...\")\n",
    "river_2020 = process_2020_river_data(table2020, slocs_with_stasiun, river_params_dict)\n",
    "print(f\"  → Shape: {river_2020.shape}\")\n",
    "print(f\"  → Date: {river_2020['tanggal'].unique()[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0578e77d",
   "metadata": {},
   "source": [
    "### Step 3: Process CSV Files (2022-2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d4d451b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[3/6] Processing CSV files...\n",
      "  → sungai2022.csv: (478, 17)\n",
      "  → sungai2023.csv: (459, 9)\n",
      "  → data-kualitas-air-sungai-komponen-data.csv: (0, 2)\n",
      "  → Combined: (937, 17)\n",
      "  → Date range: 2022-03-15 00:00:00 to 2023-09-15 00:00:00\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[3/6] Processing CSV files...\")\n",
    "data_dir = Path('data/kualitas-air-sungai')\n",
    "\n",
    "csv_files_full = [\n",
    "    (data_dir / 'sungai2022.csv', ';'),\n",
    "    (data_dir / 'sungai2023.csv', ';'),\n",
    "    (data_dir / 'data-kualitas-air-sungai-komponen-data.csv', ',')\n",
    "]\n",
    "\n",
    "river_csv_list = []\n",
    "for csv_file, delimiter in csv_files_full:\n",
    "    df_csv = process_csv_river_data(str(csv_file), gdf_boundaries, stasiun_mapping, river_params_dict, delimiter)\n",
    "    river_csv_list.append(df_csv)\n",
    "    print(f\"  → {csv_file.name}: {df_csv.shape}\")\n",
    "\n",
    "river_csv = pd.concat(river_csv_list, ignore_index=True)\n",
    "print(f\"  → Combined: {river_csv.shape}\")\n",
    "print(f\"  → Date range: {river_csv['tanggal'].min()} to {river_csv['tanggal'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13211fc6",
   "metadata": {},
   "source": [
    "### Step 4: Align Data Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "075bf1d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[4/6] Aligning data structures...\n",
      "  → PDF (2015-2019): 15 parameters\n",
      "  → 2020: 15 parameters\n",
      "  → CSV (2022-2024): 15 parameters\n",
      "  → Common across all: 15 parameters\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[4/6] Aligning data structures...\")\n",
    "\n",
    "all_params = set(river_params_dict.keys())\n",
    "pdf_params = set(river_pdf.columns) - {'tanggal', 'stasiun'}\n",
    "params_2020 = set(river_2020.columns) - {'tanggal', 'stasiun'}\n",
    "csv_params = set(river_csv.columns) - {'tanggal', 'stasiun'}\n",
    "\n",
    "print(f\"  → PDF (2015-2019): {len(pdf_params)} parameters\")\n",
    "print(f\"  → 2020: {len(params_2020)} parameters\")\n",
    "print(f\"  → CSV (2022-2024): {len(csv_params)} parameters\")\n",
    "print(f\"  → Common across all: {len(pdf_params & params_2020 & csv_params)} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6a89fd02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Column structures aligned\n"
     ]
    }
   ],
   "source": [
    "# Add missing columns\n",
    "for param in all_params:\n",
    "    if param not in river_pdf.columns:\n",
    "        river_pdf[param] = pd.NA\n",
    "    if param not in river_2020.columns:\n",
    "        river_2020[param] = pd.NA\n",
    "    if param not in river_csv.columns:\n",
    "        river_csv[param] = pd.NA\n",
    "\n",
    "# Ensure same column order\n",
    "column_order = ['tanggal', 'stasiun'] + sorted(all_params)\n",
    "river_pdf = river_pdf[column_order]\n",
    "river_2020 = river_2020[column_order]\n",
    "river_csv = river_csv[column_order]\n",
    "\n",
    "print(\"  ✓ Column structures aligned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038b414c",
   "metadata": {},
   "source": [
    "### Step 5: Combine All Data Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ef006836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[5/6] Combining all data sources...\n",
      "  → Combined shape: (1020, 17)\n",
      "  → Date range: 2015-03-15 00:00:00 to 2023-09-15 00:00:00\n",
      "  → Unique dates: 25\n",
      "  → Records by station:\n",
      "     DKI1: 74\n",
      "     DKI2: 85\n",
      "     DKI3: 208\n",
      "     DKI4: 539\n",
      "     DKI5: 114\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[5/6] Combining all data sources...\")\n",
    "\n",
    "river_all = pd.concat([river_pdf, river_2020, river_csv], ignore_index=True)\n",
    "river_all = river_all.sort_values(['tanggal', 'stasiun']).reset_index(drop=True)\n",
    "\n",
    "print(f\"  → Combined shape: {river_all.shape}\")\n",
    "print(f\"  → Date range: {river_all['tanggal'].min()} to {river_all['tanggal'].max()}\")\n",
    "print(f\"  → Unique dates: {river_all['tanggal'].nunique()}\")\n",
    "print(f\"  → Records by station:\")\n",
    "for stasiun in sorted(river_all['stasiun'].unique()):\n",
    "    count = (river_all['stasiun'] == stasiun).sum()\n",
    "    print(f\"     {stasiun}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836e335c",
   "metadata": {},
   "source": [
    "### Step 6: Create Expanded Daily Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e9f5bf8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[6/6] Creating expanded daily dataset...\n",
      "  → Skeleton: (14610, 2)\n",
      "  → Expanded: (15511, 17)\n",
      "  → Daily records per station: 3102\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[6/6] Creating expanded daily dataset...\")\n",
    "\n",
    "years = river_all['tanggal'].dt.year.unique()\n",
    "skeleton = create_date_skeleton(years, list(stasiun_mapping.keys()))\n",
    "print(f\"  → Skeleton: {skeleton.shape}\")\n",
    "\n",
    "value_cols = [col for col in river_all.columns if col not in ['tanggal', 'stasiun']]\n",
    "river_expanded = expand_temporal_data(river_all, skeleton, value_cols)\n",
    "\n",
    "print(f\"  → Expanded: {river_expanded.shape}\")\n",
    "print(f\"  → Daily records per station: {len(river_expanded) // len(stasiun_mapping)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771e88a0",
   "metadata": {},
   "source": [
    "### Pipeline Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7fa12724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "PROCESSING COMPLETE\n",
      "======================================================================\n",
      "\n",
      "Datasets created:\n",
      "  1. river_all      : (1020, 17) (sparse, actual measurements)\n",
      "  2. river_expanded : (15511, 17) (daily, filled)\n",
      "\n",
      "Date coverage: 2015-03-15 to 2023-09-15\n",
      "Stations: ['DKI1', 'DKI2', 'DKI3', 'DKI4', 'DKI5']\n",
      "Parameters: 15\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PROCESSING COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nDatasets created:\")\n",
    "print(f\"  1. river_all      : {river_all.shape} (sparse, actual measurements)\")\n",
    "print(f\"  2. river_expanded : {river_expanded.shape} (daily, filled)\")\n",
    "print(f\"\\nDate coverage: {river_all['tanggal'].min().date()} to {river_all['tanggal'].max().date()}\")\n",
    "print(f\"Stations: {sorted(river_all['stasiun'].unique())}\")\n",
    "print(f\"Parameters: {len(value_cols)}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363882ed",
   "metadata": {},
   "source": [
    "## 9. Data Quality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "67fcd132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DATA QUALITY CHECK\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"DATA QUALITY CHECK\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5d4983d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1] Missing values in river_all (sparse data):\n",
      "             Parameter  Missing Count  Missing %\n",
      "                  lead            526      51.57\n",
      "total_dissolved_solids            465      45.59\n",
      "total_suspended_solids            460      45.10\n",
      "                copper            459      45.00\n",
      "           chromium_vi            459      45.00\n",
      "chemical_oxygen_demand            459      45.00\n",
      "               cadmium            459      45.00\n",
      "                  zinc            459      45.00\n",
      "        mbas_detergent            119      11.67\n",
      "                    ph              5       0.49\n"
     ]
    }
   ],
   "source": [
    "# Missing values in sparse data\n",
    "print(\"\\n[1] Missing values in river_all (sparse data):\")\n",
    "missing_counts = river_all[value_cols].isnull().sum()\n",
    "missing_pct = (missing_counts / len(river_all) * 100).round(2)\n",
    "missing_df = pd.DataFrame({\n",
    "    'Parameter': missing_counts.index,\n",
    "    'Missing Count': missing_counts.values,\n",
    "    'Missing %': missing_pct.values\n",
    "}).sort_values('Missing %', ascending=False)\n",
    "\n",
    "print(missing_df[missing_df['Missing Count'] > 0].head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b38db66a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2] Missing values in river_expanded (daily data):\n",
      "  ✓ No missing values (forward/backward fill successful)\n"
     ]
    }
   ],
   "source": [
    "# Missing values in expanded data\n",
    "print(\"\\n[2] Missing values in river_expanded (daily data):\")\n",
    "missing_expanded = river_expanded[value_cols].isnull().sum()\n",
    "if missing_expanded.sum() > 0:\n",
    "    print(missing_expanded[missing_expanded > 0].head(10))\n",
    "else:\n",
    "    print(\"  ✓ No missing values (forward/backward fill successful)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a7dd92c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[3] Data completeness by year:\n",
      "      records  stations\n",
      "year                   \n",
      "2015       15         5\n",
      "2016       10         5\n",
      "2017       15         5\n",
      "2018       20         5\n",
      "2019       20         5\n",
      "2020        3         3\n",
      "2022      478         5\n",
      "2023      459         5\n"
     ]
    }
   ],
   "source": [
    "# Data completeness by year\n",
    "print(\"\\n[3] Data completeness by year:\")\n",
    "river_all['year'] = river_all['tanggal'].dt.year\n",
    "completeness = river_all.groupby('year').agg({\n",
    "    'tanggal': 'count',\n",
    "    'stasiun': lambda x: x.nunique()\n",
    "}).rename(columns={'tanggal': 'records', 'stasiun': 'stations'})\n",
    "print(completeness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "05cf6017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[4] Duplicate records check:\n",
      "  → Duplicate date-station pairs: 929\n",
      "\n",
      "  Sample duplicates:\n",
      "   tanggal stasiun\n",
      "2022-03-15    DKI1\n",
      "2022-03-15    DKI1\n",
      "2022-03-15    DKI1\n",
      "2022-03-15    DKI1\n",
      "2022-03-15    DKI1\n",
      "2022-03-15    DKI1\n",
      "2022-03-15    DKI1\n",
      "2022-03-15    DKI1\n",
      "2022-03-15    DKI1\n",
      "2022-03-15    DKI1\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Duplicate records\n",
    "print(\"\\n[4] Duplicate records check:\")\n",
    "duplicates = river_all[river_all.duplicated(subset=['tanggal', 'stasiun'], keep=False)]\n",
    "print(f\"  → Duplicate date-station pairs: {len(duplicates)}\")\n",
    "if len(duplicates) > 0:\n",
    "    print(\"\\n  Sample duplicates:\")\n",
    "    print(duplicates[['tanggal', 'stasiun']].head(10).to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4c8f48",
   "metadata": {},
   "source": [
    "## 10. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bb4ab4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "EXPORT COMPLETE\n",
      "======================================================================\n",
      "\n",
      "✓ Exported daily data to: river_quality_2015-2024.csv\n",
      "  Shape: (15511, 17)\n",
      "  Date range: 2015-01-01 to 2023-12-31\n",
      "  Stations: ['DKI1', 'DKI2', 'DKI3', 'DKI4', 'DKI5']\n",
      "  Parameters: 15\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "output_daily = 'river_quality_2015-2024.csv'\n",
    "river_expanded.to_csv(output_daily, index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXPORT COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n✓ Exported daily data to: {output_daily}\")\n",
    "print(f\"  Shape: {river_expanded.shape}\")\n",
    "print(f\"  Date range: {river_expanded['tanggal'].min().date()} to {river_expanded['tanggal'].max().date()}\")\n",
    "print(f\"  Stations: {sorted(river_expanded['stasiun'].unique())}\")\n",
    "print(f\"  Parameters: {len(value_cols)}\")\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
